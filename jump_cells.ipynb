{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4620e18e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Denoising del Dataset JUMP Cell con Noise2Void\n",
    "\n",
    "Il denoising di immagini microscopiche rappresenta una sfida affascinante nel campo dell’elaborazione delle immagini biologiche. Spesso, le immagini raccolte dai microscopi contengono rumore che può compromettere l’analisi successiva, e ottenere una **Ground Truth** perfetta è praticamente impossibile.  \n",
    "\n",
    "In questo notebook esploreremo come utilizzare una **rete neurale U-Net**, combinata con l’algoritmo **Noise2Void (N2V)**, per ripulire le immagini del **Dataset JUMP**, un set di immagini mediche microscopiche. Grazie a Noise2Void, è possibile estrarre immagini “pulite” senza bisogno di dati di riferimento, rendendo l’approccio ideale per dataset complessi e realistici come questo.\n",
    "\n",
    "Il nostro obiettivo è duplice:  \n",
    "- Scoprire le potenzialità delle **Deep Neural Networks** nel ridurre il rumore delle immagini.  \n",
    "- Confrontare i risultati ottenuti con approcci tradizionali di denoising, approfondendo al contempo i meccanismi interni di Noise2Void e il suo funzionamento.\n",
    "\n",
    "Il **Dataset JUMP** offre una prospettiva unica: immagini microscopiche di cellule in cui il rumore è intrinseco e inevitabile. In questo contesto, Noise2Void si distingue per la sua capacità di generare immagini pulite e affidabili, aprendo la strada a nuove possibilità nell’analisi di dati biologici complessi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab220e",
   "metadata": {},
   "source": [
    "## Download del Dataset\n",
    "\n",
    "Il **Dataset JUMP** viene scaricato in formato **.TIFF Hyperstack** dalla piattaforma **Zenodo**. È composto da **517 immagini**, ciascuna organizzata su **4 canali**, per un totale di **2068 immagini** con dimensioni di **540x540 pixel**.  \n",
    "\n",
    "I quattro canali separano le componenti **RGB e Alpha**. Questo è particolarmente utile nell’analisi cellulare, perché permette di distinguere con maggiore precisione le diverse parti della cellula e le loro colorazioni, facilitando la successiva elaborazione e visualizzazione.\n",
    "\n",
    "Dopo il download, il dataset viene suddiviso in due insiemi principali: **Training Data** e **Validation Data**.  \n",
    "- I **Training Data** vengono utilizzati dalla rete neurale per apprendere i valori ottimali dei suoi parametri.  \n",
    "- I **Validation Data** servono invece a monitorare l’andamento dell’addestramento e prevenire fenomeni di **overfitting**, garantendo che la rete generalizzi bene su dati mai visti.\n",
    "\n",
    "In questo progetto abbiamo scelto uno **split ratio di 0.8**, cioè l’**80% delle immagini** viene destinato al training, mentre il **20% rimanente** costituisce il validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c433865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from careamics import CAREamist\n",
    "from careamics.config import create_n2v_configuration\n",
    "from careamics.utils.metrics import scale_invariant_psnr\n",
    "\n",
    "sys.path.append('library')\n",
    "\n",
    "import library.dataset as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c58c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/paoloparati/Desktop/PROGETTO/source/notebooks/data/jump/noisy.tiff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82628/3935787203.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Download the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#await dataset.load_jump_dataset(\"notebooks/data/jump/noisy.tiff\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Split the Dataset into Training and Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_jump_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"notebooks/data/jump/noisy.tiff\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/PROGETTO/source/library/dataset.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(path, split_ratio)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_jump_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Load and Split the Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtifffile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset size is {len(dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0msplit_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/n2v/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(files, aszarr, key, series, level, squeeze, maxworkers, mode, name, offset, size, pattern, axesorder, categories, imread, sort, container, chunkshape, dtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         if isinstance(files, str) or not isinstance(\n\u001b[1;32m   1060\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         ):\n\u001b[0;32m-> 1062\u001b[0;31m             with TiffFile(\n\u001b[0m\u001b[1;32m   1063\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/n2v/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, file, mode, name, offset, size, omexml, _multifile, _useframes, _parent, **is_flags)\u001b[0m\n\u001b[1;32m   3989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3991\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'invalid mode {mode!r}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3993\u001b[0;31m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3994\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3995\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multifile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_multifile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_multifile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/n2v/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, file, mode, name, offset, size)\u001b[0m\n\u001b[1;32m  13925\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13926\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13927\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNullContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 13929\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  13930\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/n2v/lib/python3.10/site-packages/tifffile/tifffile.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  13940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13941\u001b[0m             \u001b[0;31m# file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13942\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 13944\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  13945\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13946\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  13947\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/paoloparati/Desktop/PROGETTO/source/notebooks/data/jump/noisy.tiff'"
     ]
    }
   ],
   "source": [
    "#Download the Dataset\n",
    "await dataset.load_jump_dataset(\"notebooks/data/jump/noisy.tiff\")\n",
    "\n",
    "#Split the Dataset into Training and Validation\n",
    "dataset.split_jump_dataset(\"notebooks/data/jump/noisy.tiff\", split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c0893",
   "metadata": {},
   "source": [
    "## Visualizzazione del Dataset\n",
    "\n",
    "Per comprendere meglio la struttura del **Dataset JUMP**, iniziamo visualizzando alcune immagini a scopo illustrativo.  \n",
    "\n",
    "Per ciascuna immagine vengono mostrati i **quattro canali** che la compongono, permettendo di osservare separatamente le componenti RGB e Alpha. Questo passaggio ci aiuta a familiarizzare con i dettagli visivi delle cellule e a capire come la rete neurale dovrà interpretare le diverse informazioni presenti in ciascun canale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tifffile.imread(\"notebooks/data/jump/noisy_train.tiff\")\n",
    "val_images = tifffile.imread(\"notebooks/data/jump/noisy_val.tiff\")\n",
    "\n",
    "starting_index = 0 #Change this to show different images\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(15, 15))\n",
    "\n",
    "for idx in range(starting_index, starting_index + 4): #Couples the axes with the indexes\n",
    "    for ax, ch in zip(axes[idx].flat, range(4)):\n",
    "        ax.imshow(train_images[idx][ch], cmap=\"gray\")\n",
    "        ax.set_title(f\"Training Image {idx} - Channel {ch+1}\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45137038",
   "metadata": {},
   "source": [
    "## Creazione della Configurazione di Training\n",
    "\n",
    "Prima di avviare l’addestramento della rete neurale, è necessario definire un oggetto di **configurazione** che specifichi i parametri principali sia della rete stessa sia del processo di training.  \n",
    "\n",
    "Tra i parametri più importanti troviamo:  \n",
    "\n",
    "- **batch_size**: indica quante immagini vengono elaborate insieme in un singolo passo dell’addestramento.  \n",
    "- **num_epochs**: definisce il numero di volte in cui l’intero dataset viene passato attraverso la rete durante l’addestramento.  \n",
    "\n",
    "È fondamentale impostare questi valori in base alle **caratteristiche dell’hardware** a disposizione. Un batch troppo grande potrebbe esaurire la memoria della GPU, mentre un numero di epoche troppo basso potrebbe impedire alla rete di apprendere in modo efficace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f29e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = create_n2v_configuration(\n",
    "    experiment_name=\"jump_cells_n2v\",\n",
    "    data_type=\"array\",\n",
    "    axes=\"SCYX\",\n",
    "    n_channels=4,\n",
    "    patch_size=(64, 64),\n",
    "    batch_size=32,\n",
    "    num_epochs=1,\n",
    "    independent_channels=True,\n",
    ")\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7d81a",
   "metadata": {},
   "source": [
    "## Addestramento del Modello\n",
    "\n",
    "L'addestramento del modello viene effettuato creando un oggetto **CAREamist** a partire dalla configurazione definita in precedenza.  \n",
    "\n",
    "Il metodo `.train` del modello richiede semplicemente di fornire le **immagini di Training** e le **immagini di Validation**. Durante il processo di training, è possibile monitorare diversi parametri che ne descrivono l’andamento, come la perdita (loss) e altre metriche di performance, per valutare come la rete sta imparando dai dati.  \n",
    "\n",
    "Al termine dell’addestramento, gli **stati migliori del modello** vengono salvati in file appositi con estensione **.ckpt (Checkpoint)**. Questi checkpoint permettono di riprendere il training in un secondo momento o di utilizzare direttamente il modello addestrato per il denoising delle immagini.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96305f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceding, make sure your GPU is available to PyTorch or the training will be very slow\n",
    "\n",
    "careamist = CAREamist(source=config, work_dir=\"notebooks/models/jump\")\n",
    "\n",
    "# train model\n",
    "print(f\"Training starting now...\")\n",
    "careamist.train(train_source=train_images, val_source=val_images)\n",
    "print(\"Training ended!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54d7c5",
   "metadata": {},
   "source": [
    "## Generazione delle Predizioni\n",
    "\n",
    "Una volta completato l’addestramento, possiamo osservare come si comporta il modello chiedendogli di **predire le immagini pulite** a partire da alcuni campioni rumorosi.  \n",
    "\n",
    "In generale, è buona pratica testare un modello su dati **diversi** da quelli utilizzati per l’addestramento. Tuttavia, nel caso dell’algoritmo **Noise2Void (N2V)** questa precauzione non è necessaria: l’algoritmo non utilizza mai le Ground Truth durante il training, quindi non ha \"visto\" le immagini in termini di valori ideali, anche se sono state impiegate per l’addestramento.  \n",
    "\n",
    "Utilizzando il metodo `.predict` del modello, sfruttiamo l’**ultimo checkpoint disponibile** per generare le predizioni delle prime 10 immagini del dataset. Le immagini risultanti vengono poi salvate in memoria in formato **.TIFF**, pronte per essere analizzate e confrontate con le versioni rumorose originali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"notebooks/predictions/jump/predictions.tiff\"\n",
    "predict_counter = 10 # The number of images we want to predict\n",
    "\n",
    "predictions = []\n",
    "for i in range(predict_counter):\n",
    "    print(f\"Predicting batch number {i}\")\n",
    "    pred_batch = careamist.predict(source=train_images[i], data_type='array', axes='CYX')\n",
    "    predictions.append(pred_batch)\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0).squeeze()\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "tifffile.imwrite(output_path, predictions)\n",
    "print(f\"TIFF file saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987037c",
   "metadata": {},
   "source": [
    "## Visualizzazione delle Predizioni\n",
    "\n",
    "A questo punto possiamo osservare i risultati delle predizioni generate dal modello.  \n",
    "\n",
    "Per semplicità, mostriamo solo la **prima immagine** del dataset, visualizzandone separatamente i **quattro canali**. Questo permette di apprezzare con maggiore dettaglio come il modello abbia trattato le diverse componenti della cellula.\n",
    "\n",
    "Inoltre, quando disponibili, confrontiamo le predizioni ottenute con un numero di epoche di **num_epochs=1** rispetto a **num_epochs=100**. Questo confronto evidenzia come l’addestramento più lungo influenzi la qualità delle immagini generate.\n",
    "\n",
    "Le immagini risultanti rappresentano in modo chiaro sia le **potenzialità dell’algoritmo Noise2Void**, capace di rimuovere il rumore senza Ground Truth, sia i **limiti empirici** che emergono quando il rumore o le strutture complesse non possono essere completamente ricostruite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_predictions_path = \"notebooks/predictions/jump/predictions100.tiff\"\n",
    "if os.path.exists(better_predictions_path):\n",
    "    better_predictions = tifffile.imread(better_predictions_path)\n",
    "else:\n",
    "    better_predictions = None\n",
    "\n",
    "fig, ax = plt.subplots(4, 3 if better_predictions is not None else 2, figsize=(15, 20))\n",
    "for i in range(4):\n",
    "    ax[i, 0].imshow(train_images[0, i], cmap=\"gray\")\n",
    "    ax[i, 0].set_title(f\"Noisy - Channel {i}\")\n",
    "    ax[i, 1].imshow(predictions[0].squeeze()[i], cmap=\"gray\")\n",
    "    ax[i, 1].set_title(\"Prediction 1 Epoch\")\n",
    "    if (better_predictions is not None):\n",
    "        ax[i, 2].imshow(better_predictions[0].squeeze()[i], cmap=\"gray\")\n",
    "        ax[i, 2].set_title(\"Prediction 100 Epoch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
